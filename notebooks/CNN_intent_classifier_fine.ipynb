{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellId": "r8hfmyfi9ta3c77ncpn93q"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "PROJ_PATH = \"../..\"\n",
    "SRC_PATH = os.path.join(PROJ_PATH, \"intent_complex_model\")\n",
    "sys.path.append(SRC_PATH)\n",
    "\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "import logging\n",
    "import math\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "\n",
    "import compress_fasttext\n",
    "import spacy\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from src.data.text_augmentation import augment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellId": "qivfp4byzmkgenfua1q2mb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!L\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellId": "dixhwhm4tyb14j6jpr4j13"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(SRC_PATH, \"Data\")\n",
    "RAW_PATH = os.path.join(DATA_PATH, \"raw\")\n",
    "PROCESSED_PATH = os.path.join(DATA_PATH, \"processed\")\n",
    "INTERIM_PATH = os.path.join(DATA_PATH, \"interim\")\n",
    "MODELS_PATH = os.path.join(SRC_PATH, \"models\")\n",
    "# Pretrained FastText embeddings can be downloaded from\n",
    "# http://files.deeppavlov.ai/embeddings/ft_native_300_ru_twitter_nltk_word_tokenize.bin\n",
    "FASTTEXT_PATH = os.path.join(MODELS_PATH, 'compressed_ft_native_300_ru_twitter_nltk_word_tokenize.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "cellId": "56iuo7vqun3upkw9ouik4f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–Ω–µ –ø–æ–º–æ–≥–∞–µ—Ç</td>\n",
       "      <td>default_problem_tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–Ω–µ –ø—Ä–∏—Ö–æ–¥–∏—Ç</td>\n",
       "      <td>default_problem_tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text                intent\n",
       "0  –Ω–µ –ø–æ–º–æ–≥–∞–µ—Ç  default_problem_tech\n",
       "1  –Ω–µ –ø—Ä–∏—Ö–æ–¥–∏—Ç  default_problem_tech"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!L\n",
    "# train = pd.read_csv(os.path.join(PROCESSED_PATH, 'intents_train_context.csv'))\n",
    "train = pd.read_excel(os.path.join(RAW_PATH, 'gipoteza_1407.xlsx'))[['phrases', 'intent']]\n",
    "train.columns = ['text', 'intent']\n",
    "train = train[~train.intent.apply(lambda x: 'default_agreement_faq' in x)]\n",
    "train = train[train.intent.apply(lambda x: 'cntxt' not in x)]\n",
    "\n",
    "\n",
    "valid = pd.read_csv(os.path.join(PROCESSED_PATH, 'intents_valid.csv'))\n",
    "test = pd.read_csv(os.path.join(PROCESSED_PATH, 'intents_test.csv'))\n",
    "test = test[test.intent.isin(train.intent.unique())]\n",
    "train = train[~train.text.isin(valid.text)]\n",
    "train = train[~train.text.isin(test.text)]\n",
    "# train.drop_duplicates(inplace=True)\n",
    "train = train.reset_index(drop=True)\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = {\n",
    "    'employee_transfer': 'support_chat_faq',\n",
    "    'default_problem_tech': 'default_error_tech'\n",
    "}\n",
    "train['intent'] = train.intent.replace(changes)\n",
    "test['intent'] = test.intent.replace(changes)\n",
    "valid['intent'] = valid.intent.replace(changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "credit_card_grace_cashtrans_faq     294\n",
       "service_close_faq                   275\n",
       "account_cashorder_faq               271\n",
       "account_avbalance_faq               268\n",
       "account_notenter_faq                249\n",
       "default_moneycheeting_tech          241\n",
       "card_hasabonuses_info               227\n",
       "account_brokerage_transfer_faq      219\n",
       "credit_card_installment_faq         211\n",
       "card_debit_prioritypass_tech        205\n",
       "default_greeting_faq                193\n",
       "card_cashback_metro_faq             182\n",
       "credit_loanrefusal_faq              146\n",
       "insurance_contractnumber_tech       105\n",
       "card_cobrand_miles_term_faq          80\n",
       "default_unsuccessfultransfer_faq     71\n",
       "default_error_tech                   61\n",
       "default_registration_office_faq      33\n",
       "default_problem_transfer_tech        31\n",
       "default_questionbot_faq               8\n",
       "Name: intent, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.intent.value_counts()[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 5 13 80\n",
      "Fri Nov 12 09:16:58 2021\n",
      "Fri Nov 12 09:16:58 2021\n",
      "Fri Nov 12 09:16:58 2021\n",
      "Fri Nov 12 09:16:58 2021\n",
      "Fri Nov 12 09:16:58 2021\n"
     ]
    }
   ],
   "source": [
    "#!L\n",
    "very_small_intents = list(train.intent.value_counts()[train.intent.value_counts()<40].index)\n",
    "med_small_intents = list(train.intent.value_counts()[(train.intent.value_counts()>=40)&(train.intent.value_counts()<150)].index)\n",
    "small_intents = list(train.intent.value_counts()[(train.intent.value_counts()>=150)&(train.intent.value_counts()<300)].index)\n",
    "big_intents = list(train.intent.value_counts()[(train.intent.value_counts()>=300)].index)\n",
    "print(len(very_small_intents), len(med_small_intents), len(small_intents), len(big_intents))\n",
    "\n",
    "print(time.ctime())\n",
    "vs = augment_df(train[train.intent.isin(very_small_intents)], frac=15, bert_path=None)\n",
    "print(time.ctime())\n",
    "ms = augment_df(train[train.intent.isin(med_small_intents)], frac=2.5, bert_path=None)\n",
    "print(time.ctime())\n",
    "ms = augment_df(ms, frac=2, bert_path=None)\n",
    "print(time.ctime())\n",
    "s = augment_df(train[train.intent.isin(small_intents)], frac=3, bert_path=None)\n",
    "print(time.ctime())\n",
    "b = train[train.intent.isin(big_intents)].copy()\n",
    "\n",
    "train = pd.concat([vs, ms, s, b])\n",
    "train.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((86528, 2), 101, 92, 92)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, train.intent.nunique(), test.intent.nunique(), valid.intent.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "cellId": "hy7kw15pdsh4sryko83wkc"
   },
   "outputs": [],
   "source": [
    "ft = compress_fasttext.models.CompressedFastTextKeyedVectors.load(FASTTEXT_PATH)\n",
    "EMBED_DIM = ft.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = max(train.text.apply(lambda x: len(word_tokenize(x))))\n",
    "print(MAX_LEN)\n",
    "\n",
    "def get_ft_embedding(txt, ft=ft, max_len=MAX_LEN):\n",
    "    tokens = word_tokenize(txt)\n",
    "    \n",
    "    if len(tokens) > MAX_LEN:\n",
    "        tokens = tokens[:MAX_LEN]\n",
    "    \n",
    "    else:\n",
    "        tokens = ['<PAD>'] * (max_len - len(tokens)) + tokens\n",
    "    \n",
    "    return np.stack(map(ft.get_vector, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "cellId": "3oi6cbfdf0nysdr7d2vtfq"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "def evaluate(data, model, criterion, device, batch_size, best_f1=False):\n",
    "    \"\"\"\n",
    "    Evaluation, return accuracy and loss\n",
    "    \"\"\"\n",
    "    total_loss = 0.\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    model.eval()  # Set mode to evaluation to disable dropout & freeze BN\n",
    "    data_loader = DataLoader(data, batch_size=batch_size)\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            output = model(x_batch)\n",
    "            total_loss += criterion(output, y_batch)\n",
    "            y_pred.extend(sigmoid(output).cpu().numpy())  # don't forget to execute sigmoid function on logits\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "    y_true = np.asarray(y_true, dtype=np.uint8)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    # finding the best threshold with highest f1 score\n",
    "    if best_f1:\n",
    "        thresholds = np.linspace(0.2, 1, 80)\n",
    "        f1s = [skm.f1_score(np.argmax(y_true, axis=1), np.array(y_pred > thr, dtype=np.uint8), average='macro') for thr in thresholds]\n",
    "        best_index = np.argmax(f1s)\n",
    "        return {\n",
    "            'f1': f1s[best_index],\n",
    "            'threshold': thresholds[best_index],\n",
    "            'loss': total_loss / len(data)\n",
    "        }\n",
    "    else:\n",
    "        accuracy = skm.f1_score(np.argmax(y_true, axis=1), np.argmax(y_pred, axis=1), average='micro')\n",
    "        f1 = skm.f1_score(np.argmax(y_true, axis=1), np.argmax(y_pred, axis=1), average='macro', zero_division=1)\n",
    "        f1_val = skm.f1_score(\n",
    "            np.argmax(y_true, axis=1),\n",
    "            np.argmax(y_pred, axis=1),\n",
    "            average='macro',\n",
    "            zero_division=1,\n",
    "            labels=lb.transform(valid.intent.unique()).argmax(axis=1),\n",
    "        )\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1,\n",
    "            'f1_val': f1_val,\n",
    "            'loss': total_loss / len(data)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "cellId": "755lnzhw4tl3k7grz0r3tc"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "def predict_proba(data, model, device, batch_size):\n",
    "    \"\"\"\n",
    "    Prediction, return numpy matrix of predictions (batch_size * n_classes)\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "\n",
    "    model.eval()  # Set mode to evaluation to disable dropout & freeze BN\n",
    "    data_loader = DataLoader(data, batch_size=batch_size)\n",
    "    softmax = nn.Softmax()\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            output = model(x_batch)\n",
    "            y_pred.extend(softmax(output).cpu().numpy())  # don't forget to execute sigmoid function on logits\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "cellId": "7u0en94t8j7h0hprimggy"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "class ModuleParallel(nn.Module):\n",
    "    \"\"\"\n",
    "    Execute multiple modules on the same input and concatenate the results\n",
    "    \"\"\"\n",
    "    def __init__(self, modules: list, axis=1):\n",
    "        super().__init__()\n",
    "        self.modules_ = nn.ModuleList(modules)\n",
    "        self.axis = axis\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.cat([m(input) for m in self.modules_], self.axis)\n",
    "\n",
    "\n",
    "class GlobalMaxPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.max(2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "cellId": "ru8wbkndxsirbpa069f54"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Identify whether metric has not been improved for certain number of epochs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 mode: str = 'min',\n",
    "                 min_delta: float = 0,\n",
    "                 patience: int = 20):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "\n",
    "        self.is_better = None\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda *_: True\n",
    "        else:\n",
    "            self._init_is_better(mode, min_delta)\n",
    "\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "\n",
    "    def step(self, current) -> bool:\n",
    "        \"\"\"\n",
    "        Make decision whether to stop training\n",
    "\n",
    "        :param current: new metric value\n",
    "        :return: whether to stop\n",
    "        \"\"\"\n",
    "        if isinstance(current, torch.Tensor):\n",
    "            current = current.cpu()\n",
    "        if np.isnan(current):\n",
    "            return True\n",
    "\n",
    "        if self.best is None:\n",
    "            self.best = current\n",
    "        else:\n",
    "            if self.is_better(current, self.best):\n",
    "                self.num_bad_epochs = 0\n",
    "                self.best = current\n",
    "            else:\n",
    "                self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if mode == 'min':\n",
    "            self.is_better = lambda value, best: value < best - min_delta\n",
    "        if mode == 'max':\n",
    "            self.is_better = lambda value, best: value > best + min_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "cellId": "wmao2shhpljpi78h1uiwia"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "class CNNTextClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN-based text classifier\n",
    "\n",
    "    It can be used for both multi-class and multi-label classification problem,\n",
    "     because loss is not specified\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_classes,\n",
    "                 embed_dim=EMBED_DIM,\n",
    "                 filters=(600,),\n",
    "                 kernel_sizes=(4,),\n",
    "                 pooling_dropout=0.8,\n",
    "                 dense_sizes=(1000,),\n",
    "                 dense_dropout=0.8,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        :param num_classes: number of outputs (classes)\n",
    "        :param word_to_id: dictionary used to compose lookup table\n",
    "        :param use_pretrained_word_vectors: whether to use pre-trained word vectors\n",
    "        :param word_vectors_path: path to word vectors file (should be in compatible format)\n",
    "        :param trainable_word_vectors: whether to train (change) vectors\n",
    "        :param embed_dim: embedding dimensionality in case of `use_pretrained_word_vectors=False`\n",
    "        :param filters: number of filters (output channels) for each kernel size of the 1st CNN layer\n",
    "        :param kernel_sizes: kernel sizes of the 1st CNN layer\n",
    "        :param pooling_dropout: dropout coefficient after pooling layer\n",
    "        :param dense_sizes: sizes of fully-connected layers\n",
    "        :param dense_dropout: dropout coefficient after each fully-connected layer\n",
    "        :param kwargs: ignored arguments\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "    \n",
    "        self.convs0 = ModuleParallel([\n",
    "            nn.Sequential(OrderedDict([\n",
    "                ('conv0_{}'.format(k), nn.Conv1d(embed_dim, f, k)),\n",
    "                ('conv0_{}_bn'.format(k), nn.BatchNorm1d(f)),\n",
    "                ('conv0_{}_relu'.format(k), nn.ReLU()),\n",
    "                ('conv0_{}_pool'.format(k), GlobalMaxPooling()),\n",
    "                ('conv0_{}_dp'.format(k), nn.Dropout(pooling_dropout)),\n",
    "            ]))\n",
    "            for k, f in zip(kernel_sizes, filters)\n",
    "        ])\n",
    "\n",
    "        dense_sizes_in = [sum(filters)] + list(dense_sizes)[:-1]\n",
    "        self.fcs = nn.Sequential(OrderedDict(chain(*[\n",
    "            [\n",
    "                ('fc{}'.format(i), nn.Linear(dense_sizes_in[i], dense_sizes[i])),\n",
    "                ('fc{}_bn'.format(i), nn.BatchNorm1d(dense_sizes[i])),\n",
    "                ('fc{}_relu'.format(i), nn.ReLU(inplace=True)),\n",
    "                ('fc{}_dp'.format(i), nn.Dropout(dense_dropout))\n",
    "            ] for i in range(len(dense_sizes))\n",
    "        ])))\n",
    "        self.fc_last = nn.Linear(dense_sizes[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv1d takes in (batch, channels, seq_len), but raw embedded is (batch, seq_len, channels)\n",
    "        x = self.convs0(x.permute(0, 2, 1))\n",
    "        x = self.fcs(x)\n",
    "        x = self.fc_last(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "cellId": "v2i2ggiwxbe6yndg793clc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 101\n",
      "Train dataset size: 86528\n",
      "Test dataset size: 6388\n",
      "86528/6501 - train/validation split\n"
     ]
    }
   ],
   "source": [
    "#!L\n",
    "all_labels = sorted(train.intent.unique())\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb.classes_ = all_labels\n",
    "y_train = lb.transform(train[\"intent\"].values)\n",
    "y_val = lb.transform(valid[\"intent\"].values)\n",
    "y_test = lb.transform(test[\"intent\"].values)\n",
    "\n",
    "print(\"Number of labels: {}\".format(len(all_labels)))\n",
    "print(\"Train dataset size: {}\".format(y_train.shape[0]))\n",
    "print(\"Test dataset size: {}\".format(y_test.shape[0]))\n",
    "print(\"{}/{} - train/validation split\".format(y_train.shape[0], y_val.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "cellId": "r1m8ee2hcfspk6mswbqzc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 2s, sys: 1.8 s, total: 3min 4s\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_encoded = np.stack(train[\"text\"].apply(get_ft_embedding))\n",
    "X_val_encoded = np.stack(valid[\"text\"].apply(get_ft_embedding))\n",
    "X_test_encoded = np.stack(test[\"text\"].apply(get_ft_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86528, 57, 100)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "cellId": "r1m8ee2hcfspk6mswbqzc"
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.FloatTensor(X_train_encoded), torch.FloatTensor(y_train))\n",
    "val_data = TensorDataset(torch.FloatTensor(X_val_encoded), torch.FloatTensor(y_val))\n",
    "test_data = TensorDataset(torch.FloatTensor(X_test_encoded), torch.FloatTensor(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "cellId": "qkw699kcngkech6uqb3m"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "model = CNNTextClassifier(\n",
    "        num_classes=y_train.shape[1],\n",
    "        kernel_sizes=[1, 2, 3, 4],\n",
    "        filters=[800, 800, 800, 800],\n",
    "        dense_sizes=[2000, 800],\n",
    "        pooling_dropout=0.45,\n",
    "        dense_dropout=0.45,\n",
    "        trainable_word_vectors=False\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "cellId": "d3nx4tacthvwy6x4cn5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 8,898,901 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "#!L\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "cellId": "ealz6oaki05pyspgo8smdr"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                        lr=0.00035)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='sum')   # sigmoid\n",
    "# criterion = nn.CrossEntropyLoss(reduction='sum')  # softmax\n",
    "\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     patience=4,\n",
    "#     verbose=True, \n",
    "#     factor=0.15\n",
    "# )\n",
    "scheduler  = optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.00035, max_lr=0.015, mode='triangular2', cycle_momentum=False)\n",
    "early_stopping = EarlyStopping(mode='max', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "cellId": "cqpbt0iksc714ck4urkxt"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "best_valid_f1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "cellId": "gfo3vv51o7oxh6is4ihi2e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1, Fri Nov 12 15:19:22 2021, train_accuracy: 0.8576 train_f1: 0.8190 train_f1_val: 0.8139 train_loss: 1.0344, val_accuracy: 0.8519 val_f1: 0.7239 val_f1_val: 0.7947 val_loss: 1.0397\n",
      "Epoch   2, Fri Nov 12 15:19:35 2021, train_accuracy: 0.9057 train_f1: 0.8895 train_f1_val: 0.8861 train_loss: 0.7182, val_accuracy: 0.8863 val_f1: 0.7848 val_f1_val: 0.8445 val_loss: 0.8148\n",
      "Epoch   3, Fri Nov 12 15:19:48 2021, train_accuracy: 0.9295 train_f1: 0.9196 train_f1_val: 0.9168 train_loss: 0.5401, val_accuracy: 0.8977 val_f1: 0.8017 val_f1_val: 0.8627 val_loss: 0.7135\n",
      "Epoch   4, Fri Nov 12 15:20:01 2021, train_accuracy: 0.9426 train_f1: 0.9360 train_f1_val: 0.9341 train_loss: 0.4490, val_accuracy: 0.9052 val_f1: 0.8157 val_f1_val: 0.8777 val_loss: 0.6426\n",
      "Epoch   5, Fri Nov 12 15:20:13 2021, train_accuracy: 0.9528 train_f1: 0.9482 train_f1_val: 0.9469 train_loss: 0.3994, val_accuracy: 0.9088 val_f1: 0.8281 val_f1_val: 0.8911 val_loss: 0.6353\n",
      "Epoch   6, Fri Nov 12 15:20:25 2021, train_accuracy: 0.9608 train_f1: 0.9570 train_f1_val: 0.9560 train_loss: 0.3346, val_accuracy: 0.9082 val_f1: 0.8303 val_f1_val: 0.8935 val_loss: 0.6227\n",
      "Epoch   7, Fri Nov 12 15:20:38 2021, train_accuracy: 0.9665 train_f1: 0.9638 train_f1_val: 0.9631 train_loss: 0.2939, val_accuracy: 0.9149 val_f1: 0.8313 val_f1_val: 0.8855 val_loss: 0.5854\n",
      "Epoch   8, Fri Nov 12 15:20:51 2021, train_accuracy: 0.9720 train_f1: 0.9692 train_f1_val: 0.9685 train_loss: 0.2641, val_accuracy: 0.9132 val_f1: 0.8435 val_f1_val: 0.8985 val_loss: 0.5891\n",
      "Epoch   9, Fri Nov 12 15:21:04 2021, train_accuracy: 0.9754 train_f1: 0.9730 train_f1_val: 0.9723 train_loss: 0.2303, val_accuracy: 0.9162 val_f1: 0.8480 val_f1_val: 0.9033 val_loss: 0.5758\n",
      "Epoch  10, Fri Nov 12 15:21:16 2021, train_accuracy: 0.9806 train_f1: 0.9787 train_f1_val: 0.9784 train_loss: 0.2030, val_accuracy: 0.9188 val_f1: 0.8508 val_f1_val: 0.9063 val_loss: 0.5690\n",
      "Epoch  11, Fri Nov 12 15:21:29 2021, train_accuracy: 0.9831 train_f1: 0.9814 train_f1_val: 0.9812 train_loss: 0.1838, val_accuracy: 0.9203 val_f1: 0.8543 val_f1_val: 0.9100 val_loss: 0.5825\n",
      "Epoch  12, Fri Nov 12 15:21:41 2021, train_accuracy: 0.9851 train_f1: 0.9836 train_f1_val: 0.9834 train_loss: 0.1654, val_accuracy: 0.9171 val_f1: 0.8412 val_f1_val: 0.9052 val_loss: 0.6028\n",
      "Epoch  13, Fri Nov 12 15:21:54 2021, train_accuracy: 0.9868 train_f1: 0.9854 train_f1_val: 0.9850 train_loss: 0.1510, val_accuracy: 0.9225 val_f1: 0.8443 val_f1_val: 0.9086 val_loss: 0.5814\n",
      "Epoch  14, Fri Nov 12 15:22:07 2021, train_accuracy: 0.9896 train_f1: 0.9884 train_f1_val: 0.9883 train_loss: 0.1269, val_accuracy: 0.9180 val_f1: 0.8398 val_f1_val: 0.9037 val_loss: 0.5770\n",
      "Epoch  15, Fri Nov 12 15:22:19 2021, train_accuracy: 0.9906 train_f1: 0.9896 train_f1_val: 0.9895 train_loss: 0.1207, val_accuracy: 0.9172 val_f1: 0.8434 val_f1_val: 0.9076 val_loss: 0.5875\n",
      "Epoch  16, Fri Nov 12 15:22:31 2021, train_accuracy: 0.9917 train_f1: 0.9907 train_f1_val: 0.9906 train_loss: 0.1033, val_accuracy: 0.9245 val_f1: 0.8549 val_f1_val: 0.9107 val_loss: 0.5565\n",
      "Epoch  17, Fri Nov 12 15:22:44 2021, train_accuracy: 0.9925 train_f1: 0.9912 train_f1_val: 0.9911 train_loss: 0.0996, val_accuracy: 0.9183 val_f1: 0.8493 val_f1_val: 0.9047 val_loss: 0.5951\n",
      "Epoch  18, Fri Nov 12 15:22:56 2021, train_accuracy: 0.9936 train_f1: 0.9926 train_f1_val: 0.9926 train_loss: 0.0863, val_accuracy: 0.9188 val_f1: 0.8522 val_f1_val: 0.9077 val_loss: 0.6214\n",
      "Epoch  19, Fri Nov 12 15:23:08 2021, train_accuracy: 0.9939 train_f1: 0.9931 train_f1_val: 0.9931 train_loss: 0.0822, val_accuracy: 0.9217 val_f1: 0.8528 val_f1_val: 0.9084 val_loss: 0.6220\n",
      "Epoch  20, Fri Nov 12 15:23:20 2021, train_accuracy: 0.9953 train_f1: 0.9945 train_f1_val: 0.9945 train_loss: 0.0708, val_accuracy: 0.9220 val_f1: 0.8468 val_f1_val: 0.9112 val_loss: 0.6126\n",
      "Epoch  21, Fri Nov 12 15:23:33 2021, train_accuracy: 0.9958 train_f1: 0.9952 train_f1_val: 0.9952 train_loss: 0.0657, val_accuracy: 0.9242 val_f1: 0.8473 val_f1_val: 0.9118 val_loss: 0.6080\n",
      "Epoch  22, Fri Nov 12 15:23:45 2021, train_accuracy: 0.9960 train_f1: 0.9955 train_f1_val: 0.9957 train_loss: 0.0620, val_accuracy: 0.9228 val_f1: 0.8429 val_f1_val: 0.9070 val_loss: 0.6099\n",
      "Epoch  23, Fri Nov 12 15:23:57 2021, train_accuracy: 0.9963 train_f1: 0.9956 train_f1_val: 0.9956 train_loss: 0.0558, val_accuracy: 0.9212 val_f1: 0.8453 val_f1_val: 0.9097 val_loss: 0.6499\n",
      "Epoch  24, Fri Nov 12 15:24:09 2021, train_accuracy: 0.9968 train_f1: 0.9963 train_f1_val: 0.9964 train_loss: 0.0526, val_accuracy: 0.9209 val_f1: 0.8506 val_f1_val: 0.9061 val_loss: 0.6486\n",
      "Epoch  25, Fri Nov 12 15:24:21 2021, train_accuracy: 0.9974 train_f1: 0.9971 train_f1_val: 0.9972 train_loss: 0.0449, val_accuracy: 0.9217 val_f1: 0.8525 val_f1_val: 0.9081 val_loss: 0.6356\n",
      "Epoch  26, Fri Nov 12 15:24:33 2021, train_accuracy: 0.9975 train_f1: 0.9972 train_f1_val: 0.9972 train_loss: 0.0425, val_accuracy: 0.9220 val_f1: 0.8543 val_f1_val: 0.9100 val_loss: 0.6342\n",
      "Epoch  27, Fri Nov 12 15:24:45 2021, train_accuracy: 0.9973 train_f1: 0.9970 train_f1_val: 0.9971 train_loss: 0.0433, val_accuracy: 0.9209 val_f1: 0.8512 val_f1_val: 0.9067 val_loss: 0.6426\n",
      "Epoch  28, Fri Nov 12 15:24:58 2021, train_accuracy: 0.9977 train_f1: 0.9975 train_f1_val: 0.9975 train_loss: 0.0366, val_accuracy: 0.9202 val_f1: 0.8502 val_f1_val: 0.9057 val_loss: 0.6456\n",
      "Epoch  29, Fri Nov 12 15:25:10 2021, train_accuracy: 0.9979 train_f1: 0.9976 train_f1_val: 0.9978 train_loss: 0.0355, val_accuracy: 0.9196 val_f1: 0.8429 val_f1_val: 0.9071 val_loss: 0.6977\n",
      "Epoch  30, Fri Nov 12 15:25:21 2021, train_accuracy: 0.9981 train_f1: 0.9978 train_f1_val: 0.9979 train_loss: 0.0340, val_accuracy: 0.9199 val_f1: 0.8523 val_f1_val: 0.9079 val_loss: 0.6818\n",
      "Epoch  31, Fri Nov 12 15:25:33 2021, train_accuracy: 0.9983 train_f1: 0.9981 train_f1_val: 0.9981 train_loss: 0.0304, val_accuracy: 0.9220 val_f1: 0.8544 val_f1_val: 0.9101 val_loss: 0.6807\n",
      "Finally: test_accuracy: 0.9242 test_f1: 0.8473 test_f1_val: 0.9118 test_loss: 0.6080\n"
     ]
    }
   ],
   "source": [
    "#!L\n",
    "for epoch in range(70):\n",
    "    model.train()\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    for idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(model(x_batch), y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        # clipping gradients\n",
    "        torch.nn.utils.clip_grad_norm_([p for p in model.parameters() if p.requires_grad], 1)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    train_metrics = evaluate(train_data, model, criterion, device, batch_size)\n",
    "    val_metrics = evaluate(val_data, model, criterion, device, batch_size)\n",
    "    \n",
    "    if val_metrics['f1_val'] > best_valid_f1:\n",
    "        best_valid_f1 = val_metrics['f1_val']\n",
    "        torch.save(model.state_dict(), 'tmp_cnn_st_dict.pt')\n",
    "    \n",
    "    scheduler.step(val_metrics['f1_val'])\n",
    "    \n",
    "    print('Epoch {:3}, {}, {}, {}'\n",
    "                .format(epoch + 1, time.ctime(), ' '.join(['train_{}: {:<6.4f}'.format(k, v) for k, v in train_metrics.items()]),\n",
    "                        ' '.join(['val_{}: {:<6.4f}'.format(k, v) for k, v in val_metrics.items()])))\n",
    "\n",
    "    if early_stopping.step(val_metrics['f1_val']):\n",
    "        break\n",
    "\n",
    "model.load_state_dict(torch.load('tmp_cnn_st_dict.pt'))\n",
    "        \n",
    "model.eval()\n",
    "test_metrics = evaluate(val_data, model, criterion, device, batch_size)\n",
    "print('Finally: {}'.format(' '.join(['test_{}: {:<6.4f}'.format(k, v) for k, v in test_metrics.items()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accuracy: 0.9265 val_f1_val: 0.9001\n",
    "val_accuracy: 0.9209 val_f1: 0.8237\n",
    "al_accuracy: 0.9180 val_f1: 0.8359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "cellId": "u3xd9loin3f7zwefk7fe9r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy: 0.8394\n",
      "Val f1: 0.6809\n",
      "Val f1_val: 0.7327\n",
      "Val loss: 1.1584\n"
     ]
    }
   ],
   "source": [
    "#!L\n",
    "model.eval()\n",
    "val_metrics = evaluate(val_data, model, criterion, device, batch_size)\n",
    "print('{}'.format('\\n'.join(['Val {}: {:<6.4f}'.format(k, v) for k, v in val_metrics.items()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('100821_CNN_scoring.xlsx') as writer:\n",
    "    pd.DataFrame(predict_proba(train_data, model, device, batch_size)).to_excel(writer, index=None, sheet_name='train_scores')\n",
    "    train.to_excel(writer, sheet_name='train_data', index=None)\n",
    "    \n",
    "    pd.DataFrame(predict_proba(val_data, model, device, batch_size)).to_excel(writer, index=None, sheet_name='val_scores')\n",
    "    valid.to_excel(writer, sheet_name='val_data', index=None)\n",
    "    \n",
    "    pd.DataFrame(predict_proba(test_data, model, device, batch_size)).to_excel(writer, index=None, sheet_name='test_scores')\n",
    "    test.to_excel(writer, sheet_name='test_data', index=None)\n",
    "    \n",
    "    pd.Series(lb.classes_).to_excel(writer, sheet_name='classes_', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "cellId": "5zumgv5v6pnykaeldcstkg"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "# saving model\n",
    "torch.save(model.state_dict(), os.path.join(MODELS_PATH, '4_intents_cnn_{}.pt'.format(len(lb.classes_))))\n",
    "with open(os.path.join(MODELS_PATH, 'intents_{}.txt'.format(len(lb.classes_))), 'w') as f:\n",
    "    f.write(';'.join(lb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "cellId": "4hj8o47jsyjmyv63zedlvd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!L\n",
    "# loading model\n",
    "model = CNNTextClassifier(\n",
    "        y_train.shape[1],\n",
    "        kernel_sizes=[1, 3],\n",
    "        filters=[600, 600],\n",
    "        dense_sizes=[2000, 800],\n",
    "        pooling_dropout=0.5,\n",
    "        dense_dropout=0.5,\n",
    "        trainable_word_vectors=False\n",
    "    ).to(device)\n",
    "model.load_state_dict(torch.load(os.path.join(MODELS_PATH, '2_intents_cnn_{}.pt'.format(len(lb.classes_))), map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "cellId": "542xi02i9tyxo6pifchk6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val top-3 accuracy: 0.9816844082654979\n",
      "Val top-5 accuracy: 0.9904508453350032\n",
      "Val F1-micro/Accuracy: 0.9106136505948653\n",
      "Val F1-macro: 0.8195158723932594\n",
      "Val F1-weighted: 0.91082959937294\n",
      "Val precision-macro: 0.8163827635551293\n",
      "Val recall-macro: 0.8289665895200922\n",
      "Val precision-weighted: 0.9160887854826631\n",
      "Val recall-weighted: 0.9106136505948653\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes in 'y_true' (94) not equal to the number of classes in 'y_score' (103).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-b4753794fb25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_probas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m print(\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;34m\"Test top-3 accuracy: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_k_accuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_probas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;34m\"Test top-5 accuracy: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_k_accuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_probas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;34m'Test F1-micro/Accuracy: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/arasov/lib/python3.7/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mtop_k_accuracy_score\u001b[0;34m(y_true, y_score, k, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   1664\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my_score_n_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1665\u001b[0m             raise ValueError(\n\u001b[0;32m-> 1666\u001b[0;31m                 \u001b[0;34mf\"Number of classes in 'y_true' ({n_classes}) not equal \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1667\u001b[0m                 \u001b[0;34mf\"to the number of classes in 'y_score' ({y_score_n_classes}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Number of classes in 'y_true' (94) not equal to the number of classes in 'y_score' (103)."
     ]
    }
   ],
   "source": [
    "#!L\n",
    "pred_probas_val = predict_proba(val_data, model, device, batch_size)\n",
    "preds_val = np.argmax(pred_probas_val, axis=1)\n",
    "print(\n",
    "    \"Val top-3 accuracy: {}\".format(skm.top_k_accuracy_score(np.argmax(y_val, axis=1), pred_probas_val, k=3, labels=range(len(lb.classes_)))),\n",
    "    \"Val top-5 accuracy: {}\".format(skm.top_k_accuracy_score(np.argmax(y_val, axis=1), pred_probas_val, k=5, labels=range(len(lb.classes_)))),\n",
    "    'Val F1-micro/Accuracy: {}'.format(skm.f1_score(np.argmax(y_val, axis=1), preds_val, average='micro')),\n",
    "    'Val F1-macro: {}'.format(skm.f1_score(np.argmax(y_val, axis=1), preds_val, average='macro')),\n",
    "    'Val F1-weighted: {}'.format(skm.f1_score(np.argmax(y_val, axis=1), preds_val, average='weighted')),\n",
    "    'Val precision-macro: {}'.format(skm.precision_score(np.argmax(y_val, axis=1), preds_val, average='macro')),\n",
    "    'Val recall-macro: {}'.format(skm.recall_score(np.argmax(y_val, axis=1), preds_val, average='macro')),\n",
    "    'Val precision-weighted: {}'.format(skm.precision_score(np.argmax(y_val, axis=1), preds_val, average='weighted')),\n",
    "    'Val recall-weighted: {}'.format(skm.recall_score(np.argmax(y_val, axis=1), preds_val, average='weighted')),\n",
    "    sep='\\n'\n",
    ")\n",
    "pred_probas = predict_proba(test_data, model, device, batch_size)\n",
    "preds = np.argmax(pred_probas, axis=1)\n",
    "print(\n",
    "    \"Test top-3 accuracy: {}\".format(skm.top_k_accuracy_score(np.argmax(y_test, axis=1), pred_probas, k=3, labels=range(len(lb.classes_)))),\n",
    "    \"Test top-5 accuracy: {}\".format(skm.top_k_accuracy_score(np.argmax(y_test, axis=1), pred_probas, k=5, labels=range(len(lb.classes_)))),\n",
    "    'Test F1-micro/Accuracy: {}'.format(skm.f1_score(np.argmax(y_test, axis=1), preds, average='micro')),\n",
    "    'Test F1-macro: {}'.format(skm.f1_score(np.argmax(y_test, axis=1), preds, average='macro')),\n",
    "    'Test F1-weighted: {}'.format(skm.f1_score(np.argmax(y_test, axis=1), preds, average='weighted')),\n",
    "    'Test precision-macro: {}'.format(skm.precision_score(np.argmax(y_test, axis=1), preds, average='macro')),\n",
    "    'Test recall-macro: {}'.format(skm.recall_score(np.argmax(y_test, axis=1), preds, average='macro')),\n",
    "    'Test precision-weighted: {}'.format(skm.precision_score(np.argmax(y_test, axis=1), preds, average='weighted')),\n",
    "    'Test recall-weighted: {}'.format(skm.recall_score(np.argmax(y_test, axis=1), preds, average='weighted')),\n",
    "    sep='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "cellId": "uau75gftg445m6kpy4nlh"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "#convert labels to intents\n",
    "intents_true = list(map(lambda x: lb.classes_[x], np.argmax(y_val, axis=1)))\n",
    "intents_pred = list(map(lambda x: lb.classes_[x], preds_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           precision    recall  f1-score   support\n",
      "\n",
      "       default_thanks_faq       0.95      1.00      0.98        79\n",
      "         support_chat_faq       0.60      0.73      0.66        45\n",
      "      card_operations_faq       0.96      0.94      0.95       176\n",
      "         account_fees_faq       0.98      0.78      0.87       317\n",
      "credit_card_decrlimit_faq       0.98      0.99      0.99       193\n",
      "    account_transtime_faq       0.83      0.96      0.89       125\n",
      "      default_qestion_faq       0.83      0.65      0.73        46\n",
      "   default_fraudcalls_faq       0.89      0.99      0.94        69\n",
      "       support_number_faq       0.93      0.96      0.95       117\n",
      "   credit_card_grace_info       0.96      0.92      0.94       238\n",
      "\n",
      "                micro avg       0.93      0.90      0.91      1405\n",
      "                macro avg       0.89      0.89      0.89      1405\n",
      "             weighted avg       0.93      0.90      0.91      1405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(skm.classification_report(intents_true, intents_pred, #labels=list(set(intents_true))))\n",
    "                                labels=['default_thanks_faq',\n",
    "                                        'support_chat_faq',\n",
    "                                        'card_operations_faq',\n",
    "                                        'account_fees_faq',\n",
    "                                        'credit_card_decrlimit_faq',\n",
    "                                        'account_transtime_faq',\n",
    "                                        'default_qestion_faq',\n",
    "                                        'default_fraudcalls_faq',\n",
    "                                        'support_number_faq',\n",
    "                                        'credit_card_grace_info']\n",
    "                               ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53121</th>\n",
       "      <td>—è—Å–Ω–æ –≤—Å–µ–≥–æ —Ö–æ—Ä–æ–µ–≥–æ</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79814</th>\n",
       "      <td>–ø–æ–Ω—è–ª–∞ –≤–∞—Å. —Å–ø–∞—Å–∏–±–æ –∑–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –≤—Å–µ–≥–æ –¥–æ–±—Ä–æ–≥–æ!</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53186</th>\n",
       "      <td>–∞ –æ–∫\\n—Å–ø–∞—Å–∏–±–æ \\n–ø–æ–Ω—è–ª–∞</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53258</th>\n",
       "      <td>—Å–ø–∞—Å–∏–±–æ –≤–∞–º –∑–∞ –ø–æ–º–æ—â—å</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79905</th>\n",
       "      <td>–≤—Å–µ, –ø–æ–Ω—è–ª–∞. —Å–ø–∞—Å–∏–±–æ. –µ—Å–ª–∏ –¥–æ xx –¥–Ω–µ–π, —Ç–æ —É–∂–µ ...</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52722</th>\n",
       "      <td>–∞–∞ –ø–æ–Ω—è–ª</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79511</th>\n",
       "      <td>–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52807</th>\n",
       "      <td>—Å–ø–∞—Å–∏–±–æ üôèüèª</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79682</th>\n",
       "      <td>–æ—Ç–ª–∏—á–Ω–æ, –æ–≥—Ä–æ–º–Ω–æ–µ —Å–ø–∞—Å–∏–±–æ –≤–∞–º!)))</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79650</th>\n",
       "      <td>—Å–ø–∞—Å–∏–±–æ –∑–∞ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ—Å—Ç—å. –∏ –≤–∞–º</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53478</th>\n",
       "      <td>–±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –æ—Ç–≤–µ—Ç) –∫–∞—Ä—Ç—É –Ω–µ –Ω—É–∂–Ω–æ</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53402</th>\n",
       "      <td>—Å–ø–∞—Å–∏–±–æ —è –ø–æ–Ω—è–ª –≤ —á–µ–º –¥–µ–ª–æ</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53040</th>\n",
       "      <td>–æ—Ç–ª–∏—á–Ω–æ –±–ª–∞–≥–æ–¥–∞—Ä—é</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53024</th>\n",
       "      <td>–≤—Å–µ –Ω–∞—à–ª–∞ —Å–ø–∞—Å–∏–±–æ</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79666</th>\n",
       "      <td>—Å–ø–∞—Å–∏–±–æ –±–æ–ª—å—à–æ–µ  —Ö–æ—Ä–æ—à–µ–≥–æ –¥–Ω—è ü§ó</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53602</th>\n",
       "      <td>—Å–ø–∞—Å–∏–±–æ –±–æ–ª—å—à–æ–µ –≤—ã –æ—á–µ–Ω—å –ø–æ–º–æ–≥–ª–∏ üòäüôè –æ—Ç–ª–∏—á–Ω–æ–≥–æ ...</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79846</th>\n",
       "      <td>–ø–æ–Ω—è—Ç–Ω–æ, –±–æ–ª—å—à–æ–µ —Å–ø–∞—Å–∏–±–æ –∑–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, —Ö–æ—Ä–æ—à–∏...</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52985</th>\n",
       "      <td>–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ—Å–ø–∞—Å–∏–±–æ</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52790</th>\n",
       "      <td>–≤–∏–¥–∏–º–æ –ø–æ–Ω—è–ª</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79817</th>\n",
       "      <td>–≤—Å—ë –≤–µ—Ä–Ω–æ, –±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –º–æ–ª–Ω–∏–µ–Ω–æ—Å–Ω—É—é –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω...</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52863</th>\n",
       "      <td>–±–ª–∞–≥–æ–¥–∞—Ä—é üòäüòä</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79871</th>\n",
       "      <td>–≥–¥–µ —Ç–æ —è —ç—Ç–æ —É–∂–µ –≤–∏–¥–µ–ª, –≤–∞—à–∏ 00 –¥–Ω–µ–π, –∫–∞—Ä–æ—á–µ —è...</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79567</th>\n",
       "      <td>—Å–ø–∞—Å–∏–±–æ –±–æ–ª—å—à–æ–µ. –¥–æ—Å–≤–∏–¥–∞–Ω–∏–µ</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52749</th>\n",
       "      <td>–≤—Å–µ—Å–ø–∞—Å–∏–±–æ</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52756</th>\n",
       "      <td>–æ—Ç–ª–∏—á–Ω–æ üëç</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79724</th>\n",
       "      <td>—Ö–æ—Ä–æ—à–æ, –ø–æ–ø—Ä–æ–±—É—é —á—É—Ç—å –ø–æ–∑–∂–µ. —Å–ø–∞—Å–∏–±–æ.</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53605</th>\n",
       "      <td>—Å–ø–∞—Å–∏–±–æ –æ–≥—Ä–æ–º–Ω–æ–µ –∑–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –ø–æ–≤–æ–¥—É –ø—Ä–æ—Å—Ä...</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52668</th>\n",
       "      <td>—è—Å–Ω–æüëå</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52955</th>\n",
       "      <td>–±–ª–∞–≥–æ–¥–∞—Ä—é –ø—Ä–æ–±—É—é</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52680</th>\n",
       "      <td>–æ—Ç –¥—É—à–∏</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53028</th>\n",
       "      <td>–≤—Å–µ–≥–æ –¥–æ–±—Ä–æ–≥–æ –≤–∞–º</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53451</th>\n",
       "      <td>–±–ª–∞–≥–æ–¥–∞—Ä—é —É –º–µ–Ω—è –≤—Å–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53023</th>\n",
       "      <td>–≤—Å–µ –≤–æ–ø—Ä–æ—Å —Ä–µ—à–∏–ª–∏</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52825</th>\n",
       "      <td>–ª–∞–¥–Ω–æ —Å–ø–∞—Å–∏–±–æ</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52710</th>\n",
       "      <td>—Å–ø–ø—Å–∏–±–æ</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53188</th>\n",
       "      <td>–±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –æ—Ç–≤–µ—Çüòò</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53606</th>\n",
       "      <td>–¥–æ–±—Ä—ã–π –¥–µ–Ω—å, –ø–æ–¥—Å–∫–∞–∂–∏—Ç–µ –≥–¥–µ –º–Ω–µ –≤–∑—è—Ç—å –¥–∞–Ω–Ω—ã–µ –ø...</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79723</th>\n",
       "      <td>—Ö–æ—Ä–æ—à–æ , –≤—Å—ë –ø–æ–Ω—è—Ç–Ω–æ, —Å–ø–∞—Å–∏–±–æ –≤–∞–º!!!</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53225</th>\n",
       "      <td>—Å–ø–ø—Å–∏–±–æ –≤–∞–º –∑–∞ –æ—Ç–≤–µ—Ç</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79634</th>\n",
       "      <td>—Å–ø–∞—Å–∏–±–æ –≤—Å–µ –ø–æ–Ω—è—Ç–Ω–æ —Ä–∞–∑—ä—è—Å–Ω–∏–ª–∏</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79844</th>\n",
       "      <td>—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ, —Å–ø–∞—Å–∏–±–æ –∑–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞...</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79507</th>\n",
       "      <td>–ø–æ–Ω—è–ª, —Å–ø–∞—Å–∏–±–æ –±–æ–ª—å—à–æ–µ)</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52745</th>\n",
       "      <td>–∞—Ö–∞—Ö –ø–æ–Ω—è–ª</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53169</th>\n",
       "      <td>—Å–ø–∞—Å–∏–±–æ —Ä–∞–∑–æ–±—Ä–∞–ª–∞—Å—å</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79882</th>\n",
       "      <td>–±—É–¥—É –∏–∑—É—á–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –µ—Å–ª–∏ –æ—Å—Ç–∞–Ω—É—Ç—Å—è –≤–æ–ø—Ä–æ...</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53489</th>\n",
       "      <td>–±–æ–ª—å—à–æ–µ —Å–ø–∞—Å–∏–±–æ –≤—Å–µ–≥–¥–∞ —Ç–∞–∫ –ø–æ–º–æ–≥–∞–µ—Ç–µ</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79696</th>\n",
       "      <td>–±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –ø–æ–º–æ—â—å, –ø—Ä–∏—è—Ç–Ω–æ–≥–æ –¥–Ω—è</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53525</th>\n",
       "      <td>—É—Ä–∞üòòüòòüòò –º–æ–≥—É –æ–Ω–ª–∞–π–Ω —Å–ø–∞—Å–∏–±–æ –∏ —Ö–æ—Ä –¥–µ—è</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53323</th>\n",
       "      <td>—Ö–æ—Ä–æ—à–æ –ø–æ–Ω—è—Ç–Ω–æ —Å–ø–∞—Å–∏–±–æ</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79854</th>\n",
       "      <td>–æ–∫, —Å–ø–∞—Å–∏–±–æ. —ç—Ç–æ —è –∏ —Ö–æ—Ç–µ–ª —É–∑–Ω–∞—Ç—å. –∏–∑–≤–∏–Ω—è—é—Å—å –∑...</td>\n",
       "      <td>default_thanks_faq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text              intent\n",
       "53121                                 —è—Å–Ω–æ –≤—Å–µ–≥–æ —Ö–æ—Ä–æ–µ–≥–æ  default_thanks_faq\n",
       "79814  –ø–æ–Ω—è–ª–∞ –≤–∞—Å. —Å–ø–∞—Å–∏–±–æ –∑–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –≤—Å–µ–≥–æ –¥–æ–±—Ä–æ–≥–æ!  default_thanks_faq\n",
       "53186                             –∞ –æ–∫\\n—Å–ø–∞—Å–∏–±–æ \\n–ø–æ–Ω—è–ª–∞  default_thanks_faq\n",
       "53258                              —Å–ø–∞—Å–∏–±–æ –≤–∞–º –∑–∞ –ø–æ–º–æ—â—å  default_thanks_faq\n",
       "79905  –≤—Å–µ, –ø–æ–Ω—è–ª–∞. —Å–ø–∞—Å–∏–±–æ. –µ—Å–ª–∏ –¥–æ xx –¥–Ω–µ–π, —Ç–æ —É–∂–µ ...  default_thanks_faq\n",
       "52722                                           –∞–∞ –ø–æ–Ω—è–ª  default_thanks_faq\n",
       "79511                           –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è  default_thanks_faq\n",
       "52807                                         —Å–ø–∞—Å–∏–±–æ üôèüèª  default_thanks_faq\n",
       "79682                  –æ—Ç–ª–∏—á–Ω–æ, –æ–≥—Ä–æ–º–Ω–æ–µ —Å–ø–∞—Å–∏–±–æ –≤–∞–º!)))  default_thanks_faq\n",
       "79650                    —Å–ø–∞—Å–∏–±–æ –∑–∞ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ—Å—Ç—å. –∏ –≤–∞–º  default_thanks_faq\n",
       "53478                 –±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –æ—Ç–≤–µ—Ç) –∫–∞—Ä—Ç—É –Ω–µ –Ω—É–∂–Ω–æ  default_thanks_faq\n",
       "53402                         —Å–ø–∞—Å–∏–±–æ —è –ø–æ–Ω—è–ª –≤ —á–µ–º –¥–µ–ª–æ  default_thanks_faq\n",
       "53040                                  –æ—Ç–ª–∏—á–Ω–æ –±–ª–∞–≥–æ–¥–∞—Ä—é  default_thanks_faq\n",
       "53024                                  –≤—Å–µ –Ω–∞—à–ª–∞ —Å–ø–∞—Å–∏–±–æ  default_thanks_faq\n",
       "79666                    —Å–ø–∞—Å–∏–±–æ –±–æ–ª—å—à–æ–µ  —Ö–æ—Ä–æ—à–µ–≥–æ –¥–Ω—è ü§ó  default_thanks_faq\n",
       "53602  —Å–ø–∞—Å–∏–±–æ –±–æ–ª—å—à–æ–µ –≤—ã –æ—á–µ–Ω—å –ø–æ–º–æ–≥–ª–∏ üòäüôè –æ—Ç–ª–∏—á–Ω–æ–≥–æ ...  default_thanks_faq\n",
       "79846  –ø–æ–Ω—è—Ç–Ω–æ, –±–æ–ª—å—à–æ–µ —Å–ø–∞—Å–∏–±–æ –∑–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, —Ö–æ—Ä–æ—à–∏...  default_thanks_faq\n",
       "52985                                   –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ—Å–ø–∞—Å–∏–±–æ  default_thanks_faq\n",
       "52790                                       –≤–∏–¥–∏–º–æ –ø–æ–Ω—è–ª  default_thanks_faq\n",
       "79817  –≤—Å—ë –≤–µ—Ä–Ω–æ, –±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –º–æ–ª–Ω–∏–µ–Ω–æ—Å–Ω—É—é –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω...  default_thanks_faq\n",
       "52863                                       –±–ª–∞–≥–æ–¥–∞—Ä—é üòäüòä  default_thanks_faq\n",
       "79871  –≥–¥–µ —Ç–æ —è —ç—Ç–æ —É–∂–µ –≤–∏–¥–µ–ª, –≤–∞—à–∏ 00 –¥–Ω–µ–π, –∫–∞—Ä–æ—á–µ —è...  default_thanks_faq\n",
       "79567                        —Å–ø–∞—Å–∏–±–æ –±–æ–ª—å—à–æ–µ. –¥–æ—Å–≤–∏–¥–∞–Ω–∏–µ  default_thanks_faq\n",
       "52749                                         –≤—Å–µ—Å–ø–∞—Å–∏–±–æ  default_thanks_faq\n",
       "52756                                          –æ—Ç–ª–∏—á–Ω–æ üëç  default_thanks_faq\n",
       "79724              —Ö–æ—Ä–æ—à–æ, –ø–æ–ø—Ä–æ–±—É—é —á—É—Ç—å –ø–æ–∑–∂–µ. —Å–ø–∞—Å–∏–±–æ.  default_thanks_faq\n",
       "53605  —Å–ø–∞—Å–∏–±–æ –æ–≥—Ä–æ–º–Ω–æ–µ –∑–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –ø–æ–≤–æ–¥—É –ø—Ä–æ—Å—Ä...  default_thanks_faq\n",
       "52668                                              —è—Å–Ω–æüëå  default_thanks_faq\n",
       "52955                                   –±–ª–∞–≥–æ–¥–∞—Ä—é –ø—Ä–æ–±—É—é  default_thanks_faq\n",
       "52680                                            –æ—Ç –¥—É—à–∏  default_thanks_faq\n",
       "53028                                  –≤—Å–µ–≥–æ –¥–æ–±—Ä–æ–≥–æ –≤–∞–º  default_thanks_faq\n",
       "53451                    –±–ª–∞–≥–æ–¥–∞—Ä—é —É –º–µ–Ω—è –≤—Å–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å  default_thanks_faq\n",
       "53023                                  –≤—Å–µ –≤–æ–ø—Ä–æ—Å —Ä–µ—à–∏–ª–∏  default_thanks_faq\n",
       "52825                                      –ª–∞–¥–Ω–æ —Å–ø–∞—Å–∏–±–æ  default_thanks_faq\n",
       "52710                                            —Å–ø–ø—Å–∏–±–æ  default_thanks_faq\n",
       "53188                                –±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –æ—Ç–≤–µ—Çüòò  default_thanks_faq\n",
       "53606  –¥–æ–±—Ä—ã–π –¥–µ–Ω—å, –ø–æ–¥—Å–∫–∞–∂–∏—Ç–µ –≥–¥–µ –º–Ω–µ –≤–∑—è—Ç—å –¥–∞–Ω–Ω—ã–µ –ø...  default_thanks_faq\n",
       "79723               —Ö–æ—Ä–æ—à–æ , –≤—Å—ë –ø–æ–Ω—è—Ç–Ω–æ, —Å–ø–∞—Å–∏–±–æ –≤–∞–º!!!  default_thanks_faq\n",
       "53225                               —Å–ø–ø—Å–∏–±–æ –≤–∞–º –∑–∞ –æ—Ç–≤–µ—Ç  default_thanks_faq\n",
       "79634                     —Å–ø–∞—Å–∏–±–æ –≤—Å–µ –ø–æ–Ω—è—Ç–Ω–æ —Ä–∞–∑—ä—è—Å–Ω–∏–ª–∏  default_thanks_faq\n",
       "79844  —Å–æ–≥–ª–∞—à–µ–Ω–∏–µ –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ, —Å–ø–∞—Å–∏–±–æ –∑–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞...  default_thanks_faq\n",
       "79507                            –ø–æ–Ω—è–ª, —Å–ø–∞—Å–∏–±–æ –±–æ–ª—å—à–æ–µ)  default_thanks_faq\n",
       "52745                                         –∞—Ö–∞—Ö –ø–æ–Ω—è–ª  default_thanks_faq\n",
       "53169                                —Å–ø–∞—Å–∏–±–æ —Ä–∞–∑–æ–±—Ä–∞–ª–∞—Å—å  default_thanks_faq\n",
       "79882  –±—É–¥—É –∏–∑—É—á–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –µ—Å–ª–∏ –æ—Å—Ç–∞–Ω—É—Ç—Å—è –≤–æ–ø—Ä–æ...  default_thanks_faq\n",
       "53489               –±–æ–ª—å—à–æ–µ —Å–ø–∞—Å–∏–±–æ –≤—Å–µ–≥–¥–∞ —Ç–∞–∫ –ø–æ–º–æ–≥–∞–µ—Ç–µ  default_thanks_faq\n",
       "79696                 –±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –ø–æ–º–æ—â—å, –ø—Ä–∏—è—Ç–Ω–æ–≥–æ –¥–Ω—è  default_thanks_faq\n",
       "53525               —É—Ä–∞üòòüòòüòò –º–æ–≥—É –æ–Ω–ª–∞–π–Ω —Å–ø–∞—Å–∏–±–æ –∏ —Ö–æ—Ä –¥–µ—è  default_thanks_faq\n",
       "53323                             —Ö–æ—Ä–æ—à–æ –ø–æ–Ω—è—Ç–Ω–æ —Å–ø–∞—Å–∏–±–æ  default_thanks_faq\n",
       "79854  –æ–∫, —Å–ø–∞—Å–∏–±–æ. —ç—Ç–æ —è –∏ —Ö–æ—Ç–µ–ª —É–∑–Ω–∞—Ç—å. –∏–∑–≤–∏–Ω—è—é—Å—å –∑...  default_thanks_faq"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.intent=='default_thanks_faq'].sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "cellId": "oe8h72get38t6hs6efcf4k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "#!L\n",
    "clf_report = pd.DataFrame(skm.classification_report(intents_true, intents_pred, output_dict=True)).T\\\n",
    ".drop(['accuracy', 'macro avg', 'weighted avg']).sort_values('support', ascending=False)\n",
    "print(len(clf_report[clf_report['f1-score']<0.8]))\n",
    "print(len(clf_report[clf_report['f1-score']<0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellId": "9y9lzy3g3y4s2ttt30m1u"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>default_problem_faq</th>\n",
       "      <td>0.912088</td>\n",
       "      <td>0.855670</td>\n",
       "      <td>0.882979</td>\n",
       "      <td>194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>credit_total_debt_faq</th>\n",
       "      <td>0.919540</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.855615</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>default_limit_info</th>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.808219</td>\n",
       "      <td>0.874074</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>credit_channels_faq</th>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.794521</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deposit_all_faq</th>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>demp_credit_card_decrlimit_faq</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>default_codeword_faq</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card_whatquestioncashback_faq</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temp_default_passport_update_faq</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>default_questionbot_faq</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  precision    recall  f1-score  support\n",
       "default_problem_faq                0.912088  0.855670  0.882979    194.0\n",
       "credit_total_debt_faq              0.919540  0.800000  0.855615    100.0\n",
       "default_limit_info                 0.951613  0.808219  0.874074     73.0\n",
       "credit_channels_faq                0.920635  0.794521  0.852941     73.0\n",
       "deposit_all_faq                    0.804878  0.916667  0.857143     72.0\n",
       "...                                     ...       ...       ...      ...\n",
       "demp_credit_card_decrlimit_faq     0.333333  0.500000  0.400000      2.0\n",
       "default_codeword_faq               0.000000  0.000000  0.000000      1.0\n",
       "card_whatquestioncashback_faq      0.000000  0.000000  0.000000      1.0\n",
       "temp_default_passport_update_faq   0.000000  0.000000  0.000000      1.0\n",
       "default_questionbot_faq            0.500000  1.000000  0.666667      1.0\n",
       "\n",
       "[79 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!L\n",
    "clf_report[clf_report['f1-score']<0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "53jl571irbrf70uzvvzl1k"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "# clf_report['N_samples'] = clf_report.support.values * 4\n",
    "# clf_report.rename({'support': 'N_test_samples'}, axis=1, inplace=True)\n",
    "# with pd.ExcelWriter('100221_CNN_clf_report.xlsx') as writer:\n",
    "#     clf_report.sort_values('N_samples').to_excel(writer, sheet_name='0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arasov + CUDA + Spark",
   "language": "python",
   "name": "custom_kernels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "notebookId": "f0000cd1-3b9c-4b8e-b9a0-0bd5732f012c"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
